{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# ü§ñ Using Free, Open LLMs Locally with Ollama\n",
    "\n",
    "Welcome to the workshop! In this notebook, we'll use **free, open-source Large Language Models (LLMs)** running locally on your computer ‚Äî no API keys, no cloud required!\n",
    "\n",
    "We'll use **[Ollama](https://ollama.com)**, a lightweight tool that lets you run powerful LLMs locally. Our Python code communicates with it using the `requests` library, which is built into Python.\n",
    "\n",
    "### Before you begin ‚Äî one-time setup:\n",
    "1. Download and install **Ollama** from [ollama.com](https://ollama.com)\n",
    "2. Open your **Terminal** and pull the two models used in this notebook:\n",
    "  \n",
    "   `ollama pull llama3.2`\n",
    "   \n",
    "   `ollama pull phi3:3.8b`\n",
    "   \n",
    "   `ollama pull moondream`\n",
    "   \n",
    "3. Ollama will run quietly in the background ‚Äî you're ready to go!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# üé§ Part 1: Joke Teller\n",
    "\n",
    "We'll use **Llama 3.2** and **Phi3** ‚Äî text based models ‚Äî to generate jokes on any subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready to go!\n"
     ]
    }
   ],
   "source": [
    "import requests  # Built into Python ‚Äî no install needed!\n",
    "\n",
    "print(\"‚úÖ Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc1ea3b-f175-4241-95ae-8f16ee1ce752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi3:3.8b\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    1: 'llama3.2',\n",
    "    2: 'phi3:3.8b'\n",
    "}\n",
    "\n",
    "###### Change your model choice here!\n",
    "model_choice = models[2]\n",
    "print(model_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## Step 2: Write the Joke-Telling Function\n",
    "\n",
    "This function builds a **prompt** and sends it to Ollama via an HTTP request.\n",
    "The key concept: **prompt engineering** ‚Äî how you phrase the instruction shapes the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function defined!\n"
     ]
    }
   ],
   "source": [
    "def tell_joke(subject):\n",
    "    \"\"\"\n",
    "    Generates a joke about the given subject using a local LLM via Ollama.\n",
    "\n",
    "    Parameters:\n",
    "        subject (str): The topic you want a joke about.\n",
    "\n",
    "    Returns:\n",
    "        str: A joke generated by the model.\n",
    "    \"\"\"\n",
    "    # The 'prompt' is the instruction we send to the LLM.\n",
    "    # How you phrase it greatly affects the output ‚Äî this is called 'prompt engineering'!\n",
    "    prompt = f\"Tell me a short, funny joke about {subject}. Just the joke itself, nothing else.\"\n",
    "\n",
    "    # Send an HTTP POST request to the local Ollama server\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": f\"{model_choice}\",   # The model to use\n",
    "            \"prompt\": prompt,       # Our instruction\n",
    "            \"stream\": False         # Wait for the full response before returning\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # The response is JSON ‚Äî we extract the generated text\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "print(\"‚úÖ Function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## Step 3: Tell Me a Joke! üé§\n",
    "\n",
    "Try subjects like: `cats`, `programmers`, `coffee`, `libraries`, `Mondays`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a subject for your joke:  dogs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§î Thinking of a joke...\n",
      "\n",
      "üé§ Here's a joke about 'dogs':\n",
      "\n",
      "Why don't scientists trust atoms when they are dressed in yellow? Because they make up everything! And I bet that golden retriever thinks he is just as important because his 'scientist dad'. He must be a real Einstein puppy with its tail wagging equations.\n"
     ]
    }
   ],
   "source": [
    "subject = input(\"Enter a subject for your joke: \")\n",
    "\n",
    "print(\"\\nü§î Thinking of a joke...\\n\")\n",
    "\n",
    "joke = tell_joke(subject)\n",
    "\n",
    "print(f\"üé§ Here's a joke about '{subject}':\\n\")\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## üß™ Bonus: Experiment with Prompt Engineering!\n",
    "\n",
    "| Prompt Style | Example |\n",
    "|---|---|\n",
    "| Simple ask | `\"Tell me a short, funny joke about {subject}.\"` |\n",
    "| Set a persona | `\"You are a stand-up comedian. Tell a one-liner about {subject}.\"` |\n",
    "| Specify format | `\"Write a knock-knock joke about {subject}.\"` |\n",
    "| Kid-friendly | `\"Tell me a clean, family-friendly joke about {subject}.\"` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a subject:  Thomas Jefferson\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt sent to the model:\n",
      "  'You are a stand-up comedian. Tell a clever one-liner about Thomas Jefferson. Just the joke, nothing else.'\n",
      "\n",
      "ü§î Thinking...\n",
      "\n",
      "üé§ Result:\n",
      "\"I guess you could say Thomas Jefferson was a founding father of hypocrisy because he wrote the Declaration of Independence, but his personal life was more 'founding' a marriage than a family.\"\n"
     ]
    }
   ],
   "source": [
    "subject = input(\"Enter a subject: \")\n",
    "\n",
    "# Try editing this prompt!\n",
    "custom_prompt = f\"You are a stand-up comedian. Tell a clever one-liner about {subject}. Just the joke, nothing else.\"\n",
    "\n",
    "print(f\"\\nPrompt sent to the model:\\n  '{custom_prompt}'\\n\")\n",
    "print(\"ü§î Thinking...\\n\")\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\"model\": \"llama3.2\", \"prompt\": custom_prompt, \"stream\": False}\n",
    ")\n",
    "\n",
    "print(f\"üé§ Result:\\n{response.json()['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# üñºÔ∏è Part 2: Image Alt Text Generator\n",
    "\n",
    "Now we'll use a **vision model** ‚Äî an LLM that can *see* images ‚Äî to automatically generate alt text for a folder of images.\n",
    "\n",
    "**Alt text** (alternative text) is a written description of an image used by:\n",
    "- Screen readers for visually impaired users\n",
    "- Search engines for indexing images\n",
    "- Browsers when an image fails to load\n",
    "\n",
    "### Model: `moondream`\n",
    "We're switching to **Moondream**, a tiny but capable vision model (~900 MB) designed specifically for understanding and describing images. It runs entirely on your local machine.\n",
    "\n",
    "> **Make sure you've pulled it first:**  \n",
    "> Open Terminal and run: `ollama pull moondream`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-step1",
   "metadata": {},
   "source": [
    "## Step 1: Import Additional Libraries\n",
    "\n",
    "All of these are **built into Python** ‚Äî no pip install needed:\n",
    "- `base64` ‚Äî encodes images as text so they can be sent in a JSON request\n",
    "- `pathlib` ‚Äî a clean way to work with file paths and directories\n",
    "- `IPython.display` ‚Äî lets Jupyter notebooks display images inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# requests is already imported from Part 1\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-step2",
   "metadata": {},
   "source": [
    "## Step 2: Write the Alt Text Function\n",
    "\n",
    "Vision models work differently from text models. Instead of just a prompt, we also send the **image data** (encoded in base64) alongside our instruction.\n",
    "\n",
    "The Ollama API makes this seamless ‚Äî we just add an `\"images\"` field to our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alt_text(image_path):\n",
    "    \"\"\"\n",
    "    Generates alt text for an image using a local vision LLM via Ollama.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str or Path): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A concise alt text description of the image.\n",
    "    \"\"\"\n",
    "    # Read the image file and encode it as base64\n",
    "    # Base64 converts binary image data into plain text so it can travel in JSON\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Craft a prompt asking for alt text specifically\n",
    "    prompt = (\n",
    "        \"Generate concise alt text for this image. \"\n",
    "        \"Describe what is shown clearly and objectively in one or two sentences, \"\n",
    "        \"as if writing for a screen reader. Do not include phrases like 'This image shows' ‚Äî \"\n",
    "        \"just describe the content directly.\"\n",
    "    )\n",
    "\n",
    "    # Send the prompt AND the image to the Ollama vision model\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"moondream\",       # Vision-capable model\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_data],     # List of base64-encoded images\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.json()[\"response\"].strip()\n",
    "\n",
    "print(\"‚úÖ Function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-step3",
   "metadata": {},
   "source": [
    "## Step 3: Process a Folder of Images\n",
    "\n",
    "Enter the path to a folder of images below. The script will:\n",
    "1. Find all image files (`.jpg`, `.jpeg`, `.png`, `.gif`, `.webp`)\n",
    "2. Display a preview of each image\n",
    "3. Send it to the vision model\n",
    "4. Print the generated alt text\n",
    "\n",
    "**Edit the `image_folder` path in the cell below to point to your images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è  Change this to the path of your image folder:\n",
    "image_folder = \"/path/to/your/images\"\n",
    "\n",
    "# Supported image formats\n",
    "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\"}\n",
    "\n",
    "# Find all image files in the folder\n",
    "folder = Path(image_folder)\n",
    "image_files = [f for f in sorted(folder.iterdir()) if f.suffix.lower() in IMAGE_EXTENSIONS]\n",
    "\n",
    "if not image_files:\n",
    "    print(f\"‚ö†Ô∏è  No image files found in: {image_folder}\")\n",
    "    print(\"Make sure the path is correct and the folder contains .jpg, .jpeg, .png, .gif, or .webp files.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_files)} image(s) in '{folder.name}'. Generating alt text...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, image_path in enumerate(image_files, start=1):\n",
    "        print(f\"\\n[{i}/{len(image_files)}] {image_path.name}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Display a preview of the image inline in the notebook\n",
    "        display(IPImage(filename=str(image_path), width=300))\n",
    "\n",
    "        # Generate alt text\n",
    "        print(\"ü§î Generating alt text...\")\n",
    "        alt_text = generate_alt_text(image_path)\n",
    "\n",
    "        print(f\"üìù Alt text:\\n{alt_text}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n‚úÖ Done! All images processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-takeaways",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Key Takeaways\n",
    "\n",
    "### On vision models:\n",
    "- **Vision LLMs** can process both text and images ‚Äî enabling use cases like alt text, image search, document analysis, and more.\n",
    "- Sending an image to the model is as simple as encoding it in **base64** and including it in the API request.\n",
    "- **Moondream** is tiny and purpose-built for image description. Larger vision models like `llava` or `llama3.2-vision` produce richer descriptions.\n",
    "\n",
    "### On the broader picture:\n",
    "- Both use cases in this notebook ‚Äî text and vision ‚Äî use the **same API pattern**: send a prompt, get a response.\n",
    "- The only difference is the model name and adding an `\"images\"` field.\n",
    "- Ollama supports many vision models. Browse them at [ollama.com/library](https://ollama.com/library) and filter by **Vision** capability.\n",
    "\n",
    "### Want to improve the alt text?\n",
    "Try editing the prompt in `generate_alt_text()` to change the style:\n",
    "```python\n",
    "# More detailed:\n",
    "prompt = \"Describe this image in detail for someone who cannot see it.\"\n",
    "\n",
    "# SEO-focused:\n",
    "prompt = \"Write a short, keyword-rich alt text description for this image suitable for a website.\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
